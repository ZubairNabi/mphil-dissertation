%% 
%% ACS project dissertation template. 
%% 
%% Currently designed for printing two-sided, but if you prefer to 
%% print single-sided just remove ",twoside,openright" from the 
%% \documentclass[] line below. 
%%
%%
%%   SMH, May 2010. 


\documentclass[a4paper,12pt,twoside,openright]{report}


%%
%% EDIT THE BELOW TO CUSTOMIZE
%%

\def\authorname{Zubair Nabi\xspace}
\def\authorcollege{Robinson College\xspace}
\def\authoremail{Zubair.Nabi@cl.cam.ac.uk}
\def\dissertationtitle{Mission Control: Enabling Efficient and Heterogeneous Data Transfers
in Data Intensive Computing}
\def\wordcount{0}


\usepackage{epsfig,graphicx,parskip,setspace,tabularx,xspace,epstopdf,url} 

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC) 
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
\listoffigures
\listoftables

\onehalfspacing

%% START OF MAIN TEXT 

\chapter{Introduction}
\pagenumbering{arabic} 
\setcounter{page}{1} 

Over the course of the last decade, proliferation in web applications (Facebook,
Twitter etc.), and scientific computing (sensor networks, high energy physics,
etc.) has resulted in an unparalleled surge in data on the exabyte scale.
Naturally, the need to store and analyze this data has engendered an ecosystem
of distributed file systems~\cite{Ghemawat:2003:GFS}, structured and
unstructured data stores~\cite{Chang:2006:BDS,DeCandia:2007:DAH}, and
data-intensive computing
frameworks~\cite{Dean:2004:MSD,Isard:2007:DDD,Murray:2011:CUE}. These
``shared-nothing'' frameworks are mostly run in massive data centers with tens
of thousands of servers and network elements. These data centers are
geo-located, for diversity and redundancy, and run a wide-variety of loads,
including user-facing applications, such as email, custom enterprise
applications, such as authentication services, and large-scale batch-processing
applications, such as web indexing~\cite{Benson:2010:NTC}. The goal of each data
center is to provide multi-tenant environments for energy, power, space, and
cost-efficient computation, and storage at scale~\cite{Katz:2009:TTB}.

Typically, data centers are built using commodity, off-the-shelf servers and
switches to find a sweet spot between performance and cost and leverage
economies of scale~\cite{Barroso:2003:WSP}. Servers in this model, store or
process a \emph{chunk} of data, with redundancy ensured via replication without
any cluster/data center wide memory or storage area network (SAN). Due to their
commodity nature, servers and network elements are failure-prone and a failure
in say, a \emph{core switch} can bring down the entire network. Therefore,
storage and processing frameworks use a range of methods to deal with this,
including replication, re-execution, and speculative execution. On the network
side, the infrastructure predominantly consists of TCP/IP enabled wired links
connecting cheap switches with shared memory, although
wireless~\cite{Halperin:2011:ADC} and
optical~\cite{Wang:2010:CPO,Farrington:2010:HHE} links have recently been
explored. Most data centers are provisioned with 1Gbps and 10Gbps switches with
L2 Ethernet switched fabric interconnects as opposed to fabrics from the high
performance computing community such as InfiniBand. Likewise, due to the
prohibitive cost of high-end switches and routers, bandwidth is a constrained
resource in the data center. To ensure optimum utilization, subject to
organizational goals, oversubscription factors\footnote{Oversubscription factor
can simply be defined as the amount of bandwidth seen by a particular
application as opposed to the total available
bandwidth~\cite{Al-Fares:2008:SCD}.} in real data centers vary significantly:
Some are as low as 2:1 while some can get as high as
147:1~\cite{Benson:2010:NTC}. In the cluster hierarchy, bandwidth increases from
the edge of the network towards the core (off-rack oversubscription). As a
result of off-rack oversubscription, applications are designed to keep
cross-rack communication to a minimal~\cite{Dean:2004:MSD}.

Concurrently, the emergence of cloud computing has resulted in multi-tenant data
centers in which user applications have wide-varying requirements and
communication patterns. Applications range from low-latency query jobs to
bandwidth hungry batch processing jobs~\cite{Alizadeh:2010:DCT}. In addition,
applications exhibit one-to-many, many-to-one, many-to-many communication
patterns. On the data center fabric level, to ensure better fault-tolerance,
scalability, and end-to-end bandwidth, several new
topologies~\cite{Al-Fares:2008:SCD,Guo:2008:DSF,Guo:2009:BHP,Greenberg:2009:VSF}
have been proposed to replace the status quo: a hierarchical 2/3-level tree
which connects end-hosts through \emph{top-of-rack} (or \emph{aggregation}), and
\emph{core} switches. In the same vein,
L2~\cite{Mudigonda:2010:SCD,Vattikonda:2012:PTD} and
L3/L4~\cite{Alizadeh:2010:DCT,Vasudevan:2009:SEF,Raiciu:2010:DCN,Wilson:2011:BNL,Wu:2010:IIC}
protocols, topologies and design
frameworks~\cite{Singla:2011:JND,Al-Fares:2008:SCD,Guo:2008:DSF,Guo:2009:BHP,Greenberg:2009:VSF,Mudigonda:2011:TFC,Chen:2010:GAA},
and control and virtualization
planes~\cite{NiranjanMysore:2009:PSF,Mudigonda:2011:NSM,Guo:2010:SDC,Ballani:2011:TPD,Shieh:2011:SDC,Rodrigues:2011:GSB,Al-Fares:2010:HDF}
for data center communication and resource allocation have also experienced
innovation for varying goals picked from a set of high bandwidth, fairness,
fault-tolerance, and scalability.

\section{The Case Against TCP in the Data Center}
Above all, the ossification of the Internet protocol stack at the transport and
network layer~\cite{Akhshabi:2011:ELP} has also been mimicked by data center
networks. TCP/IP has become the de-facto ``narrow-waist" of the data center
network. On the one hand, this has standardized direct communication to/from and
within the data center without any middleware, but on the other, it has also
spawned a new set of problems unique to the data center environment. In the data
center, the role of IP at the network layer is minimal, as it is only employed
for naming and addressing. The only obstacle is address assignment which needs
to be done manually to encode location/topology information within the address.
Although, manual assignment is tedious and error-prone, it can be automated by
leveraging the well-defined structure of data centers~\cite{Chen:2010:GAA}. In
contrast, TCP faces a number of pathological challenges due to a significantly
different bandwidth-delay product, round-trip time (RTT), and retransmission
timeout (RTO)~\cite{Chen:2009:UTI} than a wide area network (WAN). For example,
due to the low RTT, the congestion window for each flow is very small. As a
result, flow recovery through TCP fast retransmit is impossible, leading to poor
net throughput~\cite{Kandula:2009:NDC}. To exacerbate problems, data-intensive
computing frameworks (DICF) are bandwidth hungry and require all data to be
\emph{materialized} before the next stage can commence (For example,
\emph{shuffle} between Map and Reduce stages). In fact, data transfer between
stages can account for more than 50\% of the job completion
time~\cite{Chowdhury:2011:MDT}\textsf{[TODO: Pointer to graph that plots the CDF
of shuffle time as fraction of job time]}.
Similar to compute, memory, and disk I/O ``stragglers''~\cite{Zaharia:2008:IMP},
an imbalance in network efficiency can lead to the job completion time being
dictated by the task with the longest shuffle time.

It is important to highlight that some of these problems stated above, stem in
part due to the difference in traffic characteristics and scale between data
centers and other networks. In addition, unlike wide area network applications,
there is a tight coupling between an application's usage of network, compute,
and storage resources. In production data centers, due to wide-varying mix of
applications, congestion in the network can last from 10s to 100s of
seconds~\cite{Kandula:2009:NDC}. Moreover, during high utilization periods, the
task failure rate of batch-processing systems increases, especially during the
\emph{reduce} phase of MapReduce-style applications, by a median value of 1.1x,
due to read failures over the network. In the worst case, this can halt the
entire job if subsequent tasks are dependent on the failed ones. Furthermore,
link utilization is highest in the core of the network, but most losses occur
towards the edge~\cite{Benson:2010:NTC}. Additionally, in some cases
over-provisioning the network is overkill and good performance can be achieved
by adding a few extra links at hotspot points~\cite{Kandula:2009:FTD}. Finally,
in commodity switches the buffer pool is shared by all interfaces. As a result,
if long flows hog the memory, there is queue build up in the short flows.
In reaction, TCP reduces the window size by half based on the presence of
congestion regardless of its extent. This leads to a large mismatch between the
input rate and the capacity of the link, leading to buffer underflows and loss
of throughput~\cite{Alizadeh:2010:DCT}. Overall, traffic engineering in the data
center is a complex task due to the wide variability in flow duration, flow
size, flow arrival time, and server participation~\cite{Kandula:2009:NDC}, and
standard TCP without any global knowledge is suboptimal~\cite{Benson:2010:CFT}.

Another major shortcoming of TCP, which is enunciated in the data center, is TCP
throughput collapse, or
\emph{Incast}~\cite{Chen:2009:UTI,Vasudevan:2009:SEF,Wu:2010:IIC,Alizadeh:2010:DCT}.
As a result of which, the overall application throughput significantly goes
down, by up to 90\%~\cite{Vasudevan:2009:SEF}. Typically, to handle incast,
application level remedies are added which include varying the size of the
packets so that a large number of packets can fit the switch memory, and adding
application-level jitter to offset the packet inter-arrival time. Both of these
solutions are suboptimal, as they either, increase the number of packets, or add
unnecessary delay~\cite{Alizadeh:2010:DCT}. Finally, most applications operate
in a virtualized environment where a single host runs tens of virtual machines
(VM). This sharing of CPU by multiple VMs increases the latency experienced by
each VM, which can be orders of magnitude higher than the RTT between hosts
inside a data center~\cite{Gamage:2011:OFI,Kangarlou:2010:VIT}. This
significantly slows down the progress of TCP connections between different
applications.

For completeness, it is noteworthy that TCP was designed as a general transport
mechanism for a wide area network (WAN)~\cite{Clark:1988:DPD} and has
experienced near universal adoption in environments as diverse as
satellites~\cite{Henderson:1999:TPF}.
This is primarily due to TCP's ability to provide reliability, congestion and
flow control, and in-order packet delivery.
Moreover, its maturity makes it a ``kitchen sink'' solution for
developers~\cite{Vasudevan:2009:SEF}. Naturally, it has become the transport of
choice for the data center. But it is clear from the discussion above that TCP
is sub-optimal in a data center environment -- A fact agreed upon universally by
researchers. As a result, in recent years, a number of reverse-engineered
versions of TCP have been
proposed~\cite{Alizadeh:2010:DCT,Wu:2010:IIC,Wilson:2011:BNL,Vasudevan:2009:SEF,Chen:2009:UTI}.
Unfortunately, most solutions are tied either to the underlying topology or the
structure and traffic mix of the data center: There is no one-size-fits-all
solution.

\section{Augmenting the Narrow-waist of the Data Center}
It is clear from the discussion above that, the data center communication
optimization space is wide due to the interplay of data center structure,
traffic patterns, and application requirements. Traffic engineering that
performs optimal routing in the data center requires global state, multipath
routing, and the usage of short-term predictability for
adaptation~\cite{Benson:2010:CFT,Benson:2011:MFG}. Further, data intensive
computing frameworks (DICF) work at the level of a \emph{transfer}, which is not
covered by existing solutions which operate at the flow or packet
level~\cite{Chowdhury:2011:MDT}.
Put differently, stages between DICF are dependent on bulk \emph{transfers} of
data. These requirements suggest that there needs to be a decoupling between
data transfer policy and mechanism. Therefore, in light of these requirements,
this thesis presents the design, implementation, and evaluation of \emph{Mission
Control}, an abstraction layer which operates between the applications layer,
and network layer to choose the optimum transport protocol. Specifically, it
uses runtime parameters, user-supplied requirements, network layout, and
application semantics to dynamically choose the underlying transport protocol
and schedule data transfer. The same strategy when applied at the network layer
improves both application-level and network-level
performance~\cite{Abu-Libdeh:2010:SRF}. In addition to improving performance,
transport layer characteristics can also be chosen to improve energy
efficiency~\cite{Heller:2010:ESE} or security~\cite{bittau:the}. Adding power
constraints can greatly reduce the consumption footprint of today's data centers
where the network consumes 10-20\% of the total power~\cite{Greenberg:2008:CCR}.
Likewise, application traffic can transparently and
efficiently~\cite{bittau:the} be encrypted to ensure security in a multi-tenant
environment.

Our evaluation shows that\ldots\ldots

The rest of this document is structured as follows. \S~\ref{chapter:background}
gives an overview of the cloud computing paradigm, data intensive computing
frameworks, and application models in the cloud. Related work is summarized in
\S~\ref{chapter:relatedWork}.
\S~\ref{chapter:designImplementation} presents the design and implementation of
Mission Control. An evaluation of the network patterns of existing systems and
of Mission Control is given in \S~\ref{chapter:evaluation}. Finally,
\S~\ref{chapter:conclusion} concludes and points to future work.


\chapter{Background}\label{chapter:background}
This section first gives an overview of cloud computing
(\S~\ref{chapter:background:section:cloudComputing}), followed by an
introduction to data intensive computing frameworks
(\S~\ref{chapter:background:section:dataIntensive}), and finally, a discussion
on consistency, replication, scalability, and reliability in the context of
cloud computing (\S~\ref{chapter:background:section:consistency}).

\section{Cloud Computing}\label{chapter:background:section:cloudComputing}
Cloud computing is an all encompassing term for applications (services from the
point of view of the average user), software systems, and the hardware in
massive data centers~\cite{Armbrust:2009:ATC}. \emph{Public clouds} are used by
companies to provide utility computing in which software (SaaS), infrastructure
(IaaS), and platform (PaaS) can be a service. Similarly, organizations and
companies also maintain \emph{Private clouds} which are used internally for data
processing and storage. In the cloud computing model, users can buy \emph{X} as
a service (XaaS), where X can be software, infrastructure, and platform and
elastically increase or decrease required resources based on dynamic
requirements. This allows the user to focus on application design without having
to worry about system or network administration. The rise of cloud computing has
been fuelled by the rise of ``Big Data'' and in turn, data intensive computing.
The Cloud has become the ideal platform to store and analyze massive datasets
generated by applications as diverse as satellite imagery and web indexing.
Batch processing systems, such as MapReduce~\cite{Dean:2004:MSD}, are in turn
used to compute user queries over these datasets. Finally, users can leverage
the ``cost associativity''\footnote{Using a large number of computers for a
short time costs the same amount as using a small number for a long time.} of
the Cloud and the parallelism in the application to speed up the job completion
time. Data intensive computing frameworks are the subject of the next section.

\section{Data Intensive
Computing}\label{chapter:background:section:dataIntensive}
Google's MapReduce~\cite{Dean:2004:MSD} and its open source counterpart, Hadoop
~\cite{hadoop}, are at the forefront of data intensive computing. MapReduce is a
programming model for performing large-scale computation on a cluster of
commodity machines. It abstracts work division, communication, and fault
tolerance beneath a two function API. The user only needs to specify a
\emph{map} and \emph{reduce} function, while the framework takes care of the
rest. The architecture consists of a master node and a set of worker nodes.
The master node schedules shards of the input dataset -- stored on the
distributed filesystem -- on the worker nodes. The entire flow is two-phased:
first the map function is applied to all chunks of input data by the worker
nodes, followed by the application of the reduce function to the output of the
mappers. The entire framework operates on key-value pairs. The intermediate
key-value pairs, obtained after the map phase, are \emph{shuffled} to a set of
reduce workers based on hash partitioning. The framework also has several
optimizations such as: 1) Locality, input data is assigned on the basis of
locality, 2) Fault-tolerance, failed tasks are simply rescheduled, and 3)
Speculative re-execution, slow executing tasks are speculatively re-executed on
other nodes to improve the job completion time.

One major shortcoming of MapReduce is that it is strictly a two-phase
architecture, as a result of which, it cannot easily express multi-phase SQL
like queries. Implementing these queries is awkward in the MapReduce paradigm as
they constitute multiple MapReduce jobs, requiring external operators to merge
the output~\cite{Yang:2007:MSR}. Additionally, the framework is also handicapped
by its single input, single output restriction. These shortcomings are addressed
by Microsoft's Dryad system~\cite{Isard:2007:DDD}. Dryad is a general purpose
framework that supports the processing of algorithms with directed cyclic graph
(DAG) flows. Like MapReduce, the framework abstracts work scheduling, data
partitioning, fault tolerance, and communication beneath a data-flow execution
engine. To this end, the developer only needs to a) construct a DAG of
processing using provided primitives, and b) provide a binary executable for
nodes in the DAG. Each vertex in the graph represents the execution of some
operation over its input data while the edges between the vertices represent the
communication patterns. The framework automatically schedules each vertex to a
machine/core and sets up the communication channels.
It is important to highlight that unlike MapReduce, Dryad vertices can have
multiple inputs and outputs depending on the programming semantics. Finally,
developers can easily use high-level languages to restrict the programming
interface for a particular domain.

While MapReduce and Dryad are efficient at processing a rich set of applications
with sequential flow, they fall short of supporting iterative and recursive
applications~\cite{Bu:2010:HEI,Zaharia:2010:SCC}. In addition, these
applications, such as \emph{k}-means clustering, PageRank, etc., require a more
expressive language to define their flow. These problems are addressed by
CIEL~\cite{Murray:2011:CUE} which includes a language runtime, and execution
engine, to support large-scale distributed computing. CIEL contains two
components: 1) A scripting language, \emph{Skywriting}~\cite{Murray:2010:SCS},
and 2) A distributed execution engine. Skywriting is a Turing-complete language
that can be used to express data-dependent control flow. It provides primitives
to loop over expressions or call them recursively. In addition, scripting
primitives can be used to spawn and execute tasks, and de-reference data
(reminiscent of pointers in C/C++). Further, it provides library functions to
express the flow of other frameworks such as MapReduce, as CIEL subsumes both
MapReduce, and Dryad. Architecturally, CIEL is similar to MapReduce and Dryad,
in that it also has a single master and several worker nodes, with the former in
charge of computation state, and the latter in charge of the actual computation.
CIEL relies on three primitives--objects, references, and tasks--for dynamic
task graphs. Specifically, objects are data sequences that are used for input,
and are generated as output. Objects which have not been fully computed yet, can
be used as references. Further, tasks are the unit of computation and
scheduling. Tasks use programmatic code, or binaries to execute over all its
dependencies. Moreover, tasks can either compute all its output objects, or
alternatively, spawn other tasks to do so. Tasks can be executed in either
\emph{eager}, or \emph{lazy} fashion, with the latter being the default
strategy. Finally, CIEL uses a number of optimizations to improve performance
such as deterministic naming, and streaming.

These frameworks have also spawned high-level
languages~\cite{Olston:2008:PLN,Pike:2005:IDP,Murray:2010:SCS,Yu:2008:DSG} which
allow users to write applications which are transparently translated into a
data-flow plan and scheduled on top of them. In addition, there is a rich body
of work which aims to reduce the turn around time of these
systems~\cite{Zaharia:2010:DSS,Isard:2009:QFS,Zaharia:2008:IMP}. While these
proposals have been successful in improving scheduling and fairness, the network
utilization of these batch-processing systems has largely been ignored. A recent
system, Orchestra~\cite{Chowdhury:2011:MDT}, works at the application layer to
enable intra and inter-transfer control of two communication patterns, which lie
at the heart of batch processing systems such as MapReduce:
\emph{shuffles}, and \emph{broadcasts}. For the former it uses weighted flow
assignment (WFA) enabled by an appropriate number of multiple TCP connections
and TCP's AIMD fair sharing while for the latter it leverages a BitTorrent-like
algorithm. Architecturally, it is a centralized application layer controller for
both intra and inter-transfer communication. The former is managed by one or
more Transfer Controllers (TCs) while the latter is managed by a Inter-transfer
Controller (ITC). The TC continuously monitors the transfer and updates the set
of sources associated with each destination. TCs manage the transfer at the
granularity of flows, by choosing how many concurrent flows to open from each
node, which destinations to open them to, and when to move each chunk of data.
While the results from Orchestra are promising, it is still suboptimal due to
its reliance on TCP. In addition, frameworks like Dryad support TCP pipes, and
shared memory FIFO for communication in addition to transfer of files over the
network, while Orchestra only considers the last of these.

As mentioned earlier, these frameworks operate over large amounts of data inside
environments which are error-prone. Therefore, they have to make trade-offs
between consistency, replication, scalability, and reliability. These issues are
discussed in the next section.

\section{Consistency, Replication, Scalability, and Reliability in the
Cloud}\label{chapter:background:section:consistency}
It is a well-known principle that it is hard to build distributed systems which
are consistent, available, reliable, and scalable. In fact, according to
Brewer's CAP theorem, it is impossible to build a distributed system which has
consistency (all nodes see the same data), availability (the service is always
accessible), and partition tolerance (the services can tolerate the loss of
nodes/data) at the same time~\cite{Brewer:2000:TRD,Gilbert:2002:BCF}. Developers
have to sacrifice one of the three. Due to the failure-prone, commodity
infrastructure used to build distributed systems today, partition tolerance is a
given. Therefore, systems have to make a choice between consistency and
availability~\cite{Vogels:2009:EC}. Most cloud applications today, such as
Amazon's Dynamo~\cite{DeCandia:2007:DAH} focus on availability resulting in weak
consistency or \emph{eventual consistency}. This basically available, soft
state, eventually consistent (BASE)~\cite{Pritchett:2008:BAA} model enables
applications to function even in the face of partial failure. To this end,
applications operate on local (possibly stale) replicas, while the underlying
system asynchronously applies updates to replicas.

One way to implement replication updates is through \emph{virtual
synchrony}~\cite{Birman:1987:EVS}. In this model, processes are organized into
dynamic \emph{process groups}, and messages are exchanged between groups, not
individual processes, with ordering guarantees. From the perspective of the
application, all changes to the shared data appear to be synchronous. As process
groups can have names, they can be treated as topics in a publish-subscribe
system~\cite{Birman:2010:AHO}. In addition, processes within the group are
informed if group membership changes to ensure a consistent view of group
membership across all members. Whenever a new member joins a process group, the
current state is checkpointed and sent to the newly joined member to initialize
its group replica in one atomic operation. Processes can also send multicast
events to groups with ordering guarantees.

Segue to Isis2~\cite{Birman:2012:OCW}.

\chapter{Related Work}\label{chapter:relatedWork}

\section{Topologies}
Portland~\cite{NiranjanMysore:2009:PSF}, VL2~\cite{Greenberg:2009:VSF},
FatTree~\cite{Al-Fares:2008:SCD}, Dcell~\cite{Guo:2008:DSF},
BCube~\cite{Guo:2009:BHP}.

CamCube proposes a 3D torus topology in which each server is directly connected
to a small set of other servers, inspired by structured
overlays~\cite{Abu-Libdeh:2010:SRF}. This scheme places each server into a 3D
wrapped coordinate space and its position (\emph{x,y,z}) in the space represents
its address. As a result, the physical and virtual topology of the network is
the same and applications can infer their location through an API which exposes
coordinate space information and allows one-hop Ethernet packets. Thus,
applications can implement their own routing strategies. Mission Control can be
used on CamCube to enable applications to choose both their routing strategies
and their transport mechanism.

Although, these topologies expose full bisection bandwidth, in practice due to
multi-tenancy~\cite{Costa:2012:CEI}, and TCP's inefficiency in (a) moving away
from congested links, and (b) exploiting multiple links~\cite{Raiciu:2011:IDP},
applications do not experience the full benefit. In addition, these topologies
also increase the overall cost and wiring complexity.

\section{Global Schedulers}
PLayer~\cite{Joseph:2008:PSL}, MicroTE~\cite{Benson:2011:MFG}

\section{Transport Protocols}
Deadline-Driven Delivery (D$^3$)~\cite{Wilson:2011:BNL} is a data
center-specific version of TCP that targets applications with distributed
workflow and latency targets. These applications associate a deadline with each
network flow and the flow is only useful if the deadline is met. Unfortunately,
TCP tries to achieve global fairness and maximize throughput while remaining
agnostic to any flow specific deadline. To this end, applications expose flow
deadline and size information which is exploited by end hosts to request rates
from routers along the data path. Routers only maintain aggregate counters and
thus, do not maintain any per-flow state. As a result, they are able to allocate
sending rates to flows in order for as many deadlines to be met as possible. On
the downside, D$^3$ requires changes to applications, end-hosts, and network
elements. For data centers with customizable network elements, Mission Control
can use D$^3$ for applications that expose explicit deadline information.


Data Center TCP (DCTCP)~\cite{Alizadeh:2010:DCT} uses Explicit Congestion
Notifications (ECN) from switches to perform active queue management based
congestion control. Specifically, it uses the congestion experienced flag in
packets--set by switches whenever the buffer occupancy exceeds a small
threshold--to reduce the size of the window based on a fraction of the marked
packets. This enables DCTCP to react quickly reach to queue buildup and avoid
buffer pressure. DCTCP can only work in a network with ECN-compatible switches.
Therefore, Mission Control enables DCTCP if it detects that switches in the
network support ECN.

Structured Stream Transport (SST)~\cite{Ford:2007:SSN} finds the middle-ground
between stream and datagram transmission. It implements a hierarchical and
hereditary stream structure, that allows applications to create substreams from
existing streams without incurring startup and tear-down delay. Substreams
operate independently with their own data transfer and flow control. To ensure
inter-stream fairness, substreams share congestion control. Moreover,
``ephemeral streams'' can be used to enable datagram transmission. Finally,
applications can prioritize their streams on the fly depending on runtime
requirements. Mission Control can use SST to enable MapReduce-like systems to
prioritize their data transfers, by opening multiple substreams, and to send
lightweight control traffic using ephemeral streams.

\subsection{Incast}
Vasudevan et al.~\cite{Vasudevan:2009:SEF} propose increase the granularity of
kernel timers to enable microsecond granularity RTT estimation, which is turn
used to set the RTO. They shows that reducing the RTO can increase the goodput
of simultaneous connections and hence negate Incast. Further, making
retransmissions random, can also increase the goodput by stopping multiple flows
from timings out, backing off, and retransmitting simultaneously. The takeaway
lesson is that to avoid Incast in low-latency data center networks, RTOs should
be on the same scale as network latency. Chen et al.~\cite{Chen:2009:UTI}
complement the analysis of the previous work by defining an analytical model for
Incast. This model shows that goodput is affected by both the minimum RTO timer
value and the inter-packet wait time. Unlike, these two solutions which focus on
post hoc recovery, ICTCP~\cite{Wu:2010:IIC} aims to avoid packet losses before
Incast manifestation. To this end, ICTCP implements a window based congestion
control algorithm at the receiver side. Specifically, the available bandwidth at
the receiver is used as a trigger to perform congestion control.
The frequency of this control changes dynamically based on the queueing delay
and is larger than one RTT. Finally, connection throughput is only throttled to
avoid Incast, not to decrease application goodput. We believe that all of these
implementations can be used under Mission Control depending on the dynamics
of the load.

\section{Data Center Traffic Analysis}
~\cite{Benson:2010:NTC},,
Kandula et al.~\cite{Kandula:2009:NDC} present an analysis of the traffic of a
1500 server data center. They use lightweight measurements at end-hosts in
conjunction with application level data to note the cause and effect of network
incidents. The target applications have a work-seeks-bandwidth and
scatter-gather pattern. Their analysis shows that most traffic is exchanged
within a rack and on average, a server communicates with two servers within its
rack and four outside the rack. With respect to congestion, their study shows
that highly utilized links are prevalent and congestion can last from 10s of
seconds to 100s. In addition, most periods of congestion are short lived.
Further, during periods of congestion there is an increase in the number of work
failures. Finally, traffic patterns change both frequently and quickly over
time. Following in the footsteps of Kandula et al., Benson et
al.~\cite{Benson:2009:UDC}, present SNMP data from 19 data centers. Their
analysis shows that traffic load is high in the core and decreases towards the
edge. At the same time, link loses are higher near the edge and decrease towards
the core. In addition, a small fraction of the links account for most losses.
This observation shows that traffic can be routed along alternate routes to
avoid congestion. Finally, traffic is bursty in nature and packet inter-arrival
times during spike periods follow a lognormal distribution. In a follow-up
paper, Benson et al.~\cite{Benson:2010:NTC} augment their previous work by
studying different classes of data centers. These include, university campus,
private enter

\section{Reducing Network Traffic}
Another point in the design space is to reduce the amount of traffic that is
transferred across the network by MapReduce-like
systems~\cite{Costa:2012:CEI,Yu:2009:DAD}. This can be done, either at different
levels in the cluster topology~\cite{Yu:2009:DAD} or made a function of the
network itself as in Camdoop~\cite{Costa:2012:CEI}. On the downside, performing
rack-level aggregation can saturate the ingress link to the aggregating
server~\cite{Yu:2009:DAD}. This \emph{distributed combiner} approach is most
effective for MapReduce functions which are commutative and
associative~\cite{Dean:2004:MSD}. For other function types, Camdoop distributes
the implicit sort and merge of MapReduce across all servers. At the same time, a
custom transport layer performs global scheduling of packets. This solution is
only applicable to a CamCube topology but shows that custom transport protocols
can greatly enhance performance.

\section{Wireless and Optical Networks}
Another line of work has argued that providing full-bisection links throughout
the network is overkill as only a few hotspots exist in actual data
centers~\cite{{Halperin:2011:ADC,Kandula:2009:FTD}}. The high cost and technical
challenges of newer topologies can be avoided by provisining the network for the
average case and avoiding hotspots by adding additional links on the fly.
The key challenge is to choose the placement and duration of \emph{flyways}.
Flyways are constructed using Multi-Gigabit wireless links, controlled by a
centralized scheduler that decides the temporal and spatial placement of
wireless links based on observed traffic. In cases where the traffic has high
churn, the controller would fail to respond quickly.

\section{Energy Efficiency and Security}
ElasticTree is a system that dynamically alters the power consumption of
networking elements inside a data center based on the current
traffic~\cite{Heller:2010:ESE}. It consists of an optimizer, routing control,
and power control. Based on the topology, traffic matrix, other inputs, the
optimizer computes an optimum power network subnet which is in turn fed to the
power control and the routing control. The former toggles the power state of
network elements, and the latter chooses paths for each flow. The optimizer
provides a number of strategies which make a trade-off between scalability, and
optimality. ElasticTree can be used in conjunction with Mission Control to
choose an underlying transport protocol that behaves well with it. For example,
the performance of TCP is adversely affected if there is packet reordering due
to the splitting for a single flow across multiple links by ElasticTree.

TCPcrypt is a backwards compatible enhancement to TCP that uses aims to
efficiently provide encrypted communication, transparently to
applications~\cite{bittau:the}. To this end, it uses a custom key exchange
protocol that leverages the TCP options field. Like SSL, to reduce the cost of
connection setup for short-lived flows, it enables cryptographic state from one
TCP connection to bootstrap subsequent ones. Even for legacy applications, it
protects against passive eavesdropping. Additionally, TCPcrypt protects the
integrity of TCP sessions, and defends against attacks that change their state
or affect their performance. Finally, applications can also be made aware of the
presence of TCPcrypt to negate redundant encryption. We use TCPcrypt as a secure
transport protocol option in Mission Control.

\section{Virtualization}
Due to VM consolidation, the RTT of TCP connections to VMs increases
significantly, causing the latency of connections to go up. vSnoop is a system
that allows the driver domain of a host to send an acknowledgement on behalf of
the VMs~\cite{Kangarlou:2010:VIT}. It snoops on all traffic VM traffic and
maintains per-flow state for each TCP connection which it then uses to enable
early acknowledgement of packets. The same problem afflicts the transmit path as
well, as TCP acknowledgements can get delayed, causing the TCP congestion window
to grow slower. To mitigate this, vFlood is another extension that offloads
congestion control to the driver domain, which allows the buffering of a high
number of packets~\cite{Gamage:2011:OFI}. Regardless of the transport protocol,
Mission Control can delegate congestion control to the driver domain in
virtualized environments.

\chapter{Design and Implementation}\label{chapter:designImplementation}

\section{Design}

\section{Implementation}
Different Kernels, Isis2, Python, C\#, and IronPython.

\chapter{Evaluation}\label{chapter:evaluation}

Description of setup: LXC, OpenVSwitch, etc.

Pointer to custom MapReduce Java library written for CIEL
[https://github.com/ZubairNabi/ciel-java/tree/master/examples/src/main/java/com/asgow/ciel/examples/mapreduce].

\section{Performance of CIEL on different workloads}
\section{Performance of CIEL on different workloads using Mission Control}
\section{Performance of Isis2 stuff}

\chapter{Summary and Conclusions}\label{chapter:conclusion}

 
- Streaming, switching to shared memory pipes if the source/destination is
local. Pointer to reconfigurable I/O channels?

- Possibly merge with Mesos~\cite{Hindman:2011:MPF} and
Akaros~\cite{Rhoden:2011:IPE} or Quincy~\cite{Isard:2009:QFS}.


\appendix
\singlespacing

\bibliographystyle{unsrt} 
\bibliography{dissertation} 

\end{document}
